{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 1 Project: Building a Leaf Classification App",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardcmg7/AIND-NLP/blob/master/Week_1_Project_Building_a_Leaf_Classification_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This project is from [Abubakar Abid's](https://www.linkedin.com/in/abid15/) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/computer-vision)."
      ],
      "metadata": {
        "id": "_Aktcbb3aRNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1 Project: Building a Leaf Classification App\n",
        "\n",
        "Welcome to the first week's project for *Building Computer Vision Applications*!\n",
        "\n",
        "In this week, we are going to get familiar with the key steps of machine learning, with a particular focus on image classification. Specifically, we will cover:\n",
        "\n",
        "* finding computer vision datasets and pretrained models ðŸ“–\n",
        "* fine-tuning an image classifier model on new data ðŸ‘¾\n",
        "* building a computer vision app you can run on your phone or laptop ðŸ“·\n",
        "* measuring the performance of a classification model on test data and the real world ðŸ“ˆ"
      ],
      "metadata": {
        "id": "yJTIXeWUz_Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "GaptPrn3sURb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beans are an important cereal food crop in many parts of the world. However, certain diseases can damage bean plants, causing food shortages. As a result, it is critical to monitor the leaves of bean plants frequently and accurately. \n",
        "\n",
        "This is a great example of task where an **image classification** model can help automate this work. The concepts you will learn in this project will be generally applicable to many kinds of machine learning tasks. \n",
        "\n",
        "Our end goal, will be to build a web-application that can take in an image of a bean leaf and predict whether it is health or diseased. A prototype of an app like this: \n",
        "\n",
        "![](https://i.ibb.co/6mcXB53/image.png)"
      ],
      "metadata": {
        "id": "mBXkJ-aIuZ4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Hardware Setup & Software Libraries\n",
        "\n",
        "We will be utilizing GPUs to train our machine learning model, so we will need to make sure that our colab notebook is set up correct. Go to the menu bar and click on Runtime > Change runtime type > Hardware accelerator and **make sure it is set to GPU**. Your colab notebook may restart once you make the change.\n",
        "\n",
        "We're going to be using some fantastic open-source Python libraries to load our dataset (`datasets`), train our model (`transformers`), evaluate our model (`evaluate`), and build a demo of our model (`gradio`). So let's go ahead and install all of these libraries. "
      ],
      "metadata": {
        "id": "37YHDIHJFUHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoDxhyvfsS0N"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers evaluate gradio "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Loading a Dataset"
      ],
      "metadata": {
        "id": "5qks6gJhucIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will be using the `datasets` library, which can load tens of thousands of datasets with a single line of code. It can also be used to apply preprocessing functions. Learn more about the datasets library here: https://huggingface.co/docs/datasets/tutorial\n",
        "\n",
        "Most datasets are divided into different splits. For example, you'll often see a *training* data subset, which is used to build the model, a *validation* data subset, which is used to measure the performance of the model while it is training, and a *test* dataset which is used to measure the performance of the model at the very end of training, and is usually considered how well the model will perform in the real world (we'll come back to this).\n",
        "\n",
        "Specifically, we will be using the `beans` dataset that is available for free from the Hugging Face Hub: https://huggingface.co/datasets/beans"
      ],
      "metadata": {
        "id": "mLiCkQlguhbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the Beans Dataset**"
      ],
      "metadata": {
        "id": "d8gImG59yeZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = # FILL HERE"
      ],
      "metadata": {
        "id": "6RtYcR-Dx--o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Explore the dataset by running the cells below and answer the questions below**"
      ],
      "metadata": {
        "id": "mpvyH5NTymVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "WffqZAKMy9DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  display(dataset['train'][i]['image'])"
      ],
      "metadata": {
        "id": "9VkwmyG510st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What information do we have for each sample? [ANSWER HERE]\n",
        "* How many training samples do we have? Validation samples? Test samples? [ANSWER HERE]\n",
        "* What are the labels in this dataset? [ANSWER HERE]\n",
        "* Look at the first 10 training images, do you notice anything interesting about the images in the dataset? Are they as diverse/representative as you would expect or do they have limitations? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "jW4MX0mKzDCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Loading a Pretrained Model"
      ],
      "metadata": {
        "id": "cQ0aT2Lkxu16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the `transformers` library, which can load tens of thousands of machine learning models with a few lines of code. It can also be used to fine-tune these models. Learn more about the `transformers` library here: https://huggingface.co/docs/transformers/index\n",
        "\n",
        "Specifically, we will be using the `Vision Image Transformer` model that is available for anyone from the Hugging Face Hub: https://huggingface.co/google/vit-base-patch16-224. While the details of vision transformers are beyond the scope of this course, we will point out that they are a successor of the widely used convolutional neural network (CNN) architecture and tend to perform better than CNNs at the same tasks (image classificaiton, segmentation, etc.)\n",
        "\n",
        "Let's start by seeing how the Vision Image Transformer model performs without any further fine-tuning."
      ],
      "metadata": {
        "id": "LhROC7rh0OYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the Vision Image Transformer Model and FeatureExtractor for Inference**"
      ],
      "metadata": {
        "id": "w11HNDrr1Ags"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = # FILL HERE\n",
        "model.eval()\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "NKUBueAa07fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to load the **feature extractor** corresponding to the model, so that we can convert the input images into a feature vector that the model can take as input."
      ],
      "metadata": {
        "id": "4NfyfEXLa1Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = # FILL HERE"
      ],
      "metadata": {
        "id": "w37L3EPu3VxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Use the Vision Image Transformer Model to Make a Prediction on the Training Images**"
      ],
      "metadata": {
        "id": "qURf9dz81Pzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The documentation here may be helpful: https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTForImageClassification.forward.example"
      ],
      "metadata": {
        "id": "fU7WEPSX1Tsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we get the features corresponding to the first training image\n",
        "encoding = feature_extractor(images=dataset['train'][0]['image'], return_tensors=\"pt\")\n",
        "\n",
        "# Then pass it through the model and get a prediction\n",
        "\n",
        "######\n",
        "# FILL HERE\n",
        "######\n",
        "\n",
        "prediction = # FILL HERE\n",
        "\n",
        "print(\"Predicted class:\", model.config.id2label[prediction])"
      ],
      "metadata": {
        "id": "dzOEFMMC30a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Try running the model on the first 10 samples in the dataset. What is the most common prediction? Why do you think that is? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "3nZvjOz21grz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Finetuning Your Model on the Dataset"
      ],
      "metadata": {
        "id": "EnbteoVGzZmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Off the shelf, the Vision Image Transformer will not be usable for the task that we have in mind, since it was trained for \"general\" image classification, not for the specific categories that we would like to predict. As a result, we will need to \"fine-tune\" our model.\n",
        "\n",
        "Learn more about fine-tuning models with the `transformers` library here: https://huggingface.co/docs/transformers/training\n",
        "\n",
        "We will also need to decide which metric to use for our task. Since our task is a simple image classification task, the `accuracy` metric seems reasonable: https://huggingface.co/spaces/evaluate-metric/accuracy"
      ],
      "metadata": {
        "id": "q6qD8r1v1oZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Preprocess the Dataset**\n",
        "\n",
        "To make things faster, we are going to preprocess the entire dataset so that we convert all of the images to feature vectors. This will allow us to speed up the training as we can pass the feature vectors directly. This code has already been written for you:"
      ],
      "metadata": {
        "id": "LKAAlyqn5Onm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def transform(example_batch):\n",
        "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    inputs['labels'] = example_batch['labels']\n",
        "    return inputs\n",
        "\n",
        "prepared_ds = dataset.with_transform(transform)"
      ],
      "metadata": {
        "id": "602EirtR5Q45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the Accuracy Metric**\n",
        "\n",
        "We now have to decide on a *metric* we will use to measure the performance for our machine learning model. A natural choice for image classification is *accuracy*, which measures the percentage of images that are predicted to have the correct label. \n",
        "\n",
        "Read about the `evaluate` library, which contains many common machine learning metrics here: https://github.com/huggingface/evaluate\n",
        "\n",
        "And use the `evaluate.load()` to load the accuracy metric:"
      ],
      "metadata": {
        "id": "ak1Zet4oi8dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = # FILL HERE\n",
        "\n",
        "def compute_metrics(sample):\n",
        "    return metric.compute(\n",
        "        predictions=np.argmax(sample.predictions, axis=1), \n",
        "        references=sample.label_ids)"
      ],
      "metadata": {
        "id": "drhPTlyqi7mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Fine-Tune the Vision Image Transformer Model on the Entire Training Set**"
      ],
      "metadata": {
        "id": "GJ0Qs3aK3Lf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will take all of the code that you have written and use it to fine-tune the ViT model on the beans dataset. Simply run the code below, and your model will fine-tune for 4 epochs. On a **GPU**, this should take less than 5 minutes."
      ],
      "metadata": {
        "id": "QH1xEP7klGcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-base-beans\",\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=4,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds[\"train\"],\n",
        "    eval_dataset=prepared_ds[\"validation\"],\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "ukT6WLsu7wrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "l7iaq1eu3QVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Reporting Model Metrics"
      ],
      "metadata": {
        "id": "ckPFXvIszgKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Measure Accuracy on the Validation Dataset**"
      ],
      "metadata": {
        "id": "ieelNlQK3oPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_accuracy = # FILL HERE"
      ],
      "metadata": {
        "id": "VinBsxYZ4GOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What is the loss and the accuracy on the training set [ANSWER HERE]\n",
        "* Is there any sign of overfitting? [ANSWER HERE]\n"
      ],
      "metadata": {
        "id": "OmWWsuF_4R5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Measure Accuracy on the Test Dataset**\n"
      ],
      "metadata": {
        "id": "R7QNE0dc5JLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = # FILL HERE"
      ],
      "metadata": {
        "id": "j7TSlWgI5OTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What is your final test accuracy? [ANSWER HERE]\n"
      ],
      "metadata": {
        "id": "DfhjJ3IS5QhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Building a Demo"
      ],
      "metadata": {
        "id": "rPN5O5MczrbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high-level metric like test accuracy doesn't give us a great idea on how the model will work when presented with new data from the real world. To understand this, we will build a web-based demo that can be used on our phones or computers through a web browser to test our model.\n",
        "\n",
        "The `gradio` library lets you build web demos of machine learning models with just a few lines code. Learn more about Gradio here: https://gradio.app/getting_started/\n",
        "\n",
        "Gradio lets you build machine learning demos simply by specifying (1) a prediction function, (2) the input type and (3) the output type of your model. We have already written the prediction function here:"
      ],
      "metadata": {
        "id": "WBMiNYA_5W88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset['train'].features['labels'].names\n",
        "\n",
        "def classify(im):\n",
        "  features = feature_extractor(im, return_tensors='pt').to(device)\n",
        "  logits = model(features[\"pixel_values\"])[-1]\n",
        "  probability = torch.nn.functional.softmax(logits, dim=-1)\n",
        "  probs = probability[0].detach().cpu().numpy()\n",
        "  confidences = {label: float(probs[i]) for i, label in enumerate(labels)} \n",
        "  return confidences"
      ],
      "metadata": {
        "id": "zVq6jN8T6FjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Build a Gradio web demo of your image classifier and `launch()` it**"
      ],
      "metadata": {
        "id": "QhWOPJ2X57bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `gradio.Interface` and launch it! For image classification, the input component should be `\"image\"` and output should be a `\"label\"`"
      ],
      "metadata": {
        "id": "qMDMsc3KnS3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = # FILL HERE\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7ZKC4HEADGAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Trying your Model with \"Real World\" Data!"
      ],
      "metadata": {
        "id": "AH9_sZ_00JTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Use the share link created above to open up your app on your phone**"
      ],
      "metadata": {
        "id": "M7K7GoRa6Fin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test your model on some real images of plants -- either images you find online or those outside your house (they don't have be bean plants for this part). What do you notice about the kinds of predictions your model makes?  \n",
        "\n",
        "[ANSWER HERE]"
      ],
      "metadata": {
        "id": "qZ13z89G6juq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This project is from [Abubakar Abid's](https://www.linkedin.com/in/abid15/) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/computer-vision).\n",
        "\n",
        "<img src=\"https://slack-imgs.com/?c=1&o1=ro&url=https%3A%2F%2Fcorise.com%2F_next%2Fstatic%2Fmedia%2Fcorise.638d5854.png\" width=\"400px\">\n"
      ],
      "metadata": {
        "id": "_7f2n7MFbmA0"
      }
    }
  ]
}